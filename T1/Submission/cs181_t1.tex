\documentclass[submit]{harvardml}

% Put in your full name and email address.
\name{Christine Zhang}
\email{christinezhang@college}

% List any people you worked with.
\collaborators{None}

% You don't need to change these.
\course{CS181-S18}
\assignment{Assignment \#1}
\duedate{11:59pm February 2, 2018}

\usepackage[OT1]{fontenc}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{subfig}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{common}
\usepackage[shortlabels]{enumerate}

\usepackage[mmddyyyy,hhmmss]{datetime}

\definecolor{verbgray}{gray}{0.9}

\lstnewenvironment{csv}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  columns=fullflexible}}{}
 

\begin{document}
\begin{center}
{\Large Homework 1: Linear Regression}\\
\end{center}



\subsection*{Introduction}
This homework is on different forms of linear regression and focuses
on loss functions, optimizers, and regularization. Linear regression 
will be one of the few models that we see that has an analytical solution.
These problems focus on deriving these solutions and exploring their 
properties. 

If you find that you are having trouble with the first couple
problems, we recommend going over the fundamentals of linear algebra
and matrix calculus. We also encourage you to first read the Bishop
textbook, particularly: Section 2.3 (Properties of Gaussian
Distributions), Section 3.1 (Linear Basis Regression), and Section 3.3
(Bayesian Linear Regression). (Note that our notation is slightly different but
the underlying mathematics remains the same :).

Please type your solutions after the corresponding problems using this \LaTeX\ template, and start each problem on a new page.\\

\pagebreak 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Priors and Regularization,15pts]
In this problem we consider a model of Bayesian linear regression. Define the prior on the parameters as,
\begin{align*}
p(\boldw) = \mathcal{N}(\boldw \given \bold0, \tau_w^{-1}\ident ),
\end{align*}
where $\tau_w$ is as scalar precision hyperparameter that controls the variance of the Gaussian prior.  Define the likelihood as,
\begin{align*}
p(\boldy \given \boldX, \boldw) &= \prod_{i=1}^n \mcN(y_i \given \boldw^\trans \boldx_i, \tau_n^{-1}),
\end{align*}
where $\tau_n$ is another fixed scalar defining the variance. \\

\begin{itemize}
\item[(a)] Using the fact that the posterior is the product of the prior and the likelihood (up to a normalization constant), i.e., 
\[\arg\max_{\boldw} \ln p(\boldw \given \boldy, \boldX)= \arg\max_{\boldw} \ln p(\boldw) + \ln p(\boldy \given \boldX, \boldw).\]

\noindent Show that maximizing the log posterior is equivalent to minimizing a regularized loss function given by ${\mcL(\boldw) + \lambda \mcR(\boldw)}$, where
\begin{align*}
\mcL(\boldw) &= \frac{1}{2}\sum_{i = 1}^n (y_i - \boldw^\trans \boldx_i)^2 \\
\mcR(\boldw) &= \frac{1}{2}\boldw^\trans \boldw
\end{align*} \\

Do this by writing $\ln p(\boldw \given \boldy, \boldX)$ as a function of $\mcL(\boldw)$ and $\mcR(\boldw)$, dropping constant terms if necessary.  Conclude that maximizing this posterior is equivalent to minimizing the regularized error term given by $\mcL(\boldw) + \lambda \mcR(\boldw)$ for a $\lambda$ expressed in terms of the problem's constants.  

\item[(b)] Notice that the form of the posterior is the same as the
  form of the ridge regression loss

\[\mcL(\boldw) = (\boldy - \boldX \boldw)^\top (\boldy - \boldX
\boldw) + \lambda \boldw^\top \boldw .\]

Compute the gradient of the loss above with respect to $\boldw$.
Simplify as much as you can for full credit.  Make sure to give your
answer in vector form.

\item[(c)] Suppose that $\lambda > 0$. Knowing that $\mcL$ is a convex function
    of its arguments, conclude that a global optimizer of
    $\mcL(\boldw)$ is
    \begin{align}
      \boldw &= (\boldX^\top \boldX + \lambda \boldI)^{-1} \boldX^\top \boldy
    \end{align}

For this part of the problem, assume that the data has been centered,
that is, pre-processed such that $\frac{1}{n} \sum_{i=1}^n x_{ij} = 0
$.

\item[(d)] What might happen if the number of weights in $\boldw$ is
  greater than the number of data points $N$?  How does the
  regularization help ensure that the inverse in the solution above
  can be computed?  

\end{itemize}

\end{problem}


\subsubsection*{Solution}
\begin{enumerate}[(a)]
    \item We can treat the prior as a multivariate Normal distribution and the likelihood as a summation of independent Normal distributions after applying natural log. The summation can be pulled out because of the properties of log in line (3). By multiplying by $-1$, argmax is transformed into argmin in line (5). Lastly, we can drop constant terms as they do not impact the result when solving for the maximum/minimum value in lines (3) and (9). 
    \begin{align}
    \arg\max_{\boldw} \ln p(\boldw \given \boldy, \boldX) &= \arg\max_{\boldw} \ln p(\boldw) + \ln p(\boldy \given \boldX, \boldw) \\
    &= \arg\max_{\boldw} \ln (\det(2\pi \tau_w^{-1})^{-1/2}) + \ln (e \{-\frac{1}{2} \boldw^\top \tau_w I \boldw \})\\ \nonumber
    & \qquad + \ln (\frac{1}{\sqrt{2\pi} \tau_n^{-1}}) + \ln (\sum_i - e \{\frac{(y_i - \boldw^\top x_i)^2}{2\tau_n^{-1}}\}) \\ 
    &= \arg\max_{\boldw} \ln (e \{-\frac{1}{2} \boldw^T \tau_w I \boldw \})- \sum \ln ( e \{\frac{(y_i - \boldw^\top x_i)^2}{2\tau_n^{-1}}\}) \\
    &= \arg\max_{\boldw} -\frac{\tau_w}{2} \boldw^\top \boldw - \sum \frac{\tau_n(y_i - \boldw^\top x_i)^2}{2}\\
    &= \arg\min_{\boldw} \frac{\tau_w}{2} \boldw^\top \boldw + \sum \frac{\tau_n(y_i - \boldw^\top x_i)^2}{2}\\
    &= \arg\min_{\boldw} \tau_n( \frac{\tau_w}{2\tau_n} \boldw^\top \boldw + \sum \frac{(y_i - \boldw^\top x_i)^2}{2})\\
    &= \arg\min_{\boldw} \tau_n(\lambda \mcR(\boldw)+ \mcL(\boldw))\\
    &= \arg\min_{\boldw}\mcL(\boldw) + \lambda \mcR(\boldw)
    \end{align}
    where $\lambda$ is $\tau_w/\tau_n$ in line (9).
    \item 
    \begin{align*}
        \mcL(\boldw) &= (\boldy - \boldX \boldw)^\top (\boldy - \boldX \boldw) + \lambda \boldw^\top \boldw \\
        &= (\boldy^\top - \boldw^\top\boldX^\top ) (\boldy - \boldX \boldw) + \lambda \boldw^\top \boldw \\
        &= (\boldy^\top - \boldw^\top\boldX^\top) (\boldy - \boldX \boldw) + \lambda \boldw^\top \boldw \\
        &= \boldy^\top\boldy - \boldy^\top \boldX \boldw  -\boldw^\top \boldX^\top \boldy + \boldw^\top \boldX^\top \boldX \boldw  + 2 \lambda \boldw\\
        &= \boldy^\top\boldy -2\boldw^\top \boldX^\top \boldy + \boldw^\top \boldX^\top \boldX \boldw + 2 \lambda \boldw \\
        \triangledown \mcL(\boldw) &= -2 \boldX^\top \boldy + (\boldX^\top \boldX + (\boldX^\top \boldX)^\top) \boldw + 2 \lambda \boldw \\ 
         &= -2 \boldX^\top \boldy + 2\boldX^\top \boldX \boldw + 2 \lambda \boldw \\ 
         &= 2 ( -\boldX^\top \boldy + \boldX^\top \boldX \boldw +  \lambda \boldw) \\ 
    \end{align*}
    \item Because the data is pre-processed with mean zero, this helps the model because we know that the data has mean zero and do not need to include an offset term such as $w_0$. Furthermore, this is beneficial for computation as most models expect data to be in this format and can return more accurate fits.
    
    The global optimizer of $\mcL(\boldw)$ occurs when $\triangledown \mcL(\boldw) = 0$.
    
    \begin{align*}
        \triangledown \mcL(\boldw) &= -\boldX^\top \boldy + \boldX^\top \boldX \boldw +  \lambda \boldw = 0\\
        &(\boldX^\top \boldX + \lambda)\boldw = \boldX^\top \boldy\\
        &\boldw = (\boldX^\top \boldX + \lambda)^{-1} \boldX^\top \boldy
    \end{align*}
    
    \item If the number of weights in $\boldw$ is greater than the number of data points, then we may observe overfitting to the dataset since the many features can finely tune to the few amount of data points. Furthermore, $\boldX$ may not be full rank, meaning that the matrix is singular. In other words, the columns of $\boldX^\top \boldX$ may be linearly dependent due to the high number of features, leading to a determinant of zero. 
    
    We can address this with the regularization term because we know that $\boldX^\top \boldX$ is symmetric. We can choose $\lambda$ such that the eigenvalues are all positive. This results in a positive definite matrix that is guarenteed to be invertible.  
\end{enumerate}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection*{2. Modeling Changes in Congress}
 The objective of this problem is to learn about linear regression
 with basis functions by modeling the number of Republicans in the
 Senate. The file \verb|data/year-sunspots-republicans.csv| contains the
 data you will use for this problem.  It has three columns.  The first
 one is an integer that indicates the year.  The second is the number
 of sunspots.  The third is the number of Republicans in the Senate.
 The data file looks like this:
 \begin{csv}
Year,Sunspot_Count,Republican_Count
1960,112.3,36
1962,37.6,34
1964,10.2,32
1966,47.0,36
1968,105.9,43
1970,104.5,44
\end{csv}
and you can see plots of the data in Figures~\ref{fig:congress}
and~\ref{fig:sunspots}. 

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{data/year-republicans}
\caption{Number of Republicans in the Senate.  The horizontal axis is the year, and the vertical axis is the number of Republicans.}
\label{fig:congress}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{data/year-sunspots}
\caption{Number of sunspots by year.  The horizontal axis is the year, and the vertical axis is the number of sunspots.}
\label{fig:sunspots}
\end{figure}

Data Source: \url{http://www.realclimate.org/data/senators_sunspots.txt}

\begin{problem}[Modeling Changes in Republicans and Sunspots, 15pts] 

Implement basis function regression with ordinary least squares for
years vs. number of Republicans in the Senate. Some sample Python code
is provided in \verb|linreg.py|, which implements linear regression.
Plot the data and regression lines for the simple linear case, and for
each of the following sets of basis functions:
  % FDV: Check, are these good bases to use for this problem? Adjust number and lengthscales 
\begin{itemize}
	\item[(a)] $\phi_j(x) = x^j$ for $j=1, \ldots, 5$ 
	\item[(b)] $\phi_j(x) = \exp{\frac{-(x-\mu_j)^2}{25}}$ for $\mu_j=1960, 1965, 1970, 1975, \ldots 2010$
	\item[(c)] $\phi_j(x) = \cos(x / j)$ for $j=1, \ldots, 5$
	\item[(d)] $\phi_j(x) = \cos(x / j)$ for $j=1, \ldots, 25$
\end{itemize}
In addition to the plots, provide one or two sentences for each with
numerical support, explaining whether you think it is fitting well,
overfitting or underfitting.  If it does not fit well, provide a
sentence explaining why. A good fit should capture the most important
trends in the data.\\ 

\noindent Next, do the same for the number of sunspots vs. number of
Republicans, using data only from before 1985.  What bases provide the
best fit?  Given the quality of the fit, would you believe that the
number of sunspots controls the number of Republicans in the senate?


\end{problem}



\subsubsection*{Solution}

Mean squared error (MSE) was used to quantify the fit of the model on the dataset. The variables and MSE value are reported in the chart titles. \\

Years versus number of Republicans in the Senate:

\begin{figure}[!htbp]
\begin{center}
    \includegraphics[scale=.55]{11.png}
    \includegraphics[scale=.55]{12.png}
    \caption{Basis functions a (left) and b (right)}
\end{center}
\end{figure}

Basis function (a) has a high MSE of 17.70 with a simple curve that seems to be underfitting the data with lower bias/higher variance as it does not capture the curvature pattern. Basis function (b) performs better with a MSE of 2.26 and a model that seems to fit the data well by representing the overall shape. 

\begin{figure}[!htbp]
\begin{center}
    \includegraphics[scale=.55]{13.png}
    \includegraphics[scale=.55]{14.png}
    \caption{Basis functions c (left) and d (right)}
\end{center}
\end{figure}

Basis function (c) has the higest MSE in the comparison of years vs. number of republicans at 45.12. The curve is heavily underfitting the data, likely limited by the lack of detail captured with $j$ capped at 5. Basis function (d) seems to fit the data well with a MSE of 1.62 and captures the general wave-like trend of the data. 

Number of Sunspots versus number of Republicans in the Senate

\begin{figure}[!htbp]
\begin{center}
    \includegraphics[scale=.55]{21.png}
    \includegraphics[scale=.55]{23.png}
    \caption{Basis functions a (left) and c (right)}
\end{center}
\end{figure}

Basis function (a) is underfitting the data with MSE of 27.02. This model does not resemble the general trend of the data and yields a high MSE. Basis function (c) is a poor fit of the data, yielding a jerky curve that is not representative of the general trend, and due to the limitations of the basis function, results in a poor MSE value of 28.85. The basis function produces a curve that is both high variance and high bias.

\begin{figure}[!htbp]
\begin{center}
    \includegraphics[scale=.5]{24.png}
    \includegraphics[scale=.5]{241.png}
    \caption{Basis function d (prior-1985 data) and basis function d (all data)}
\end{center}
\end{figure}

Basis function (d) is heavily overfitting the dataset with MSE of 9.74e-24 or 0. The model is running close through all of the data points with extremely high bias/low variance. Given basis function (d), it seems just looking at the numbers that the number of sunspots could control the number of Repbulicans in the Senate. Of course, common sense tells us that this is illogical. We observe that sunspot count versus number of Republicans was only run on data prior to 1985. This shows the ability to identify false positive correlations within a dataset. If we used the full dataset, the high precision fit (albeit overfitting) we observe in (d) would not exist. Instead, we would yield a poor MSE of 25.92 if we used the full sunspot/republican dataset.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{problem}[BIC, 15pts]

Adapted from $Biophysics: Searching\ for\ Principles$ by William Bialek.\\

\noindent Consider data $\mathcal{D} = \{(x_i, y_i)\}$ where we know that

    $$y_i = f(x_i ; \bold{a}) + \epsilon_i$$

\noindent where $f(x_i ; \bold{a})$ is a function with parameters $\bold{a}$, and $\epsilon_i \sim \mathcal{N}(0,\sigma^2)$ is an additive noise term.
We assume that $f$ is a polynomial with coefficients $\mathbf{a}$.
Consider the class of all polynomials of degree $K$. Each of these polynomials can be viewed as a generative model of our data, and we seek
to choose a model that both explains our current data and is able to generalize to new data. This problem explores the use of the Bayesian Information Criterion (BIC)
for model selection. Define the $\chi^2$ (\textit{chi-squared}) error term for each model of the class as:

$$\chi^2 =  \frac{1}{\sigma^2} \sum_{i=1}^N \left( y_i - \sum_{j=0}^K a_j x_i^j \right)^2 $$

\noindent Using this notation, a formulation of the BIC can be written as: 

$$-\ln P({x_i,y_i} | \text{model class} ) \approx \frac{N}{2} \ln (2\pi \sigma^2) - \sum_{i=1}^N \ln p(x_i) + \frac{1}{2} \chi^2_{min} + \frac{K+1}{2} \ln N $$ \\

\noindent where $\chi^2_{min}(K)$ denote the minimum value of the $\chi^2$ over the set of polynomial models with $K$ parameters. 
Finally, assume that $x_i \sim Unif(-5,5)$ and that each $a_j \sim Unif(-1,1)$. Let $K_{true} = 10$.

    \begin{itemize}
    
        \item[(a)] Write code that generates $N$ data points in the following way:
            \begin{enumerate}
                \item Generate a polynomial $f(x) = \sum_{j = 0}^{K_{true}} a_j x^j$ 
                \item Sample $N$ points $x_i$
                \item Compute $y_i = f(x_i) + \epsilon_i$ where $\epsilon$ is sampled from $\mathcal{N}(0, \sigma^2 = \frac{ \max_i f(x_i) - \min_i f(x_i) }{10})$.
            \end{enumerate}
        
        \item[(b)] For a set of $y_i$ generated above and a given $K$, write a function that minimizes $\chi^2$ for a polynomial of degree $K$ by solving for $\mathbf{a}$ using
                   numpy \texttt{polyfit}.                   Check for $N=20$ that $\chi^2_{min}(K)$ is a decreasing function of $K$. 
        
        \item[(c)] For $N=20$ samples, run $500$ trials. This involves generating a new polynomial for each trial, then from that polynomial, 20 sample data points $\{(x_i,y_i)\}$. For each trial, 
                          we can calculate the optimal $K$ by minimizing BIC. Compute the mean and variance of the optimal $K$ over 500 trials.
        
        \item[(d)] For $N$ ranging from $3$ to $3 \cdot 10^4$ on a log scale (you can use the function $3*np.logspace(0,4, 40)$ as your values of $N$), 
                   compute the mean and variance of the optimal $K$ over 500 trials for each $N$. Plot your results, where the x-axis is the number of samples ($N$) on a log-scale, 
                   and the y-axis is the mean value of the optimal $K$ with error bars indicating the variance over 500 trials. Verify that minimizing the BIC controls the complexity of the fit, 
                   selecting a nontrivial optimal $K$. You should observe that the optimal K is smaller than $K_{true}$ for small data sets, and approaches $K_{true}$ as you 
                   analyze larger data sets.
        
    \end{itemize}


\end{problem}



\subsubsection*{Solution}
\begin{enumerate}[(a)]

\item See attached Python file.

\item We can verify that $\chi^2_{\min}(K)$ is a decreasing function of $K$ in the plots below (left: raw values, right: log Y scale representing $\chi^2_\min$ values). 

\begin{center}
    \includegraphics[scale = .55]{decreasing_K.png}
    \includegraphics[scale = .55]{log_chi_squared.png}
\end{center}

\item The mean of optimal $K$ is 10.07 and the variance is 0.464. See code in attached Python file.

\item In the plot below, we see $K$ tending toward $K_{true}$ which shows that minimizing BIC penalizes overly complex polynomials. The first two terms of BIC are constants and invariant. The third term of BIC represents the $\chi^2_{\min}(K)$ value. We observed in part (b) that this value decreases as a function of $K$, suggesting for larger $K$ values. The fourth term of BIC adds $\frac{K+1}{2}\ln N$ to the score and penalizes high $K$ value models, creating a trade off. As $K$ tends to $K_{true}$ for larger datasets, we observe a large drop off in $\chi^2_{\min}(K)$ at $K=10$. At this point, BIC begins to penalize more complex models and $K>K_{true}$ values start to yield higher BIC scores. This results in the pattern below with optimal $K < K_{true}$ for small data sets and $K$ approaching $K_{true}$ for larger datasets. There is initially high variance tending toward $K_{true}$ but this decreases with the additional of more data points. 

\begin{center}
    \includegraphics[scale = .6]{K_converge.png}
\end{center}
\end{enumerate}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{problem}[Calibration, 1pt]
Approximately how long did this homework take you to complete?
\end{problem}
\textbf{Answer:} 15 hours \\
\textbf{Name:} Christine Zhang \\
\textbf{Email:} christinezhang@college.harvard.edu \\
\textbf{Collaborators:} David Yang

\end{document}

